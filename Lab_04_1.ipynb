{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOH+u3RQ8D2D+uEwWOqZRs5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ii3sx-O-KxcC"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.optim as optim"]},{"cell_type":"code","source":["# Multivariate Linear Regression"],"metadata":{"id":"l5Bf63TWK2xa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"],"metadata":{"id":"2_5PHL5yo5j5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 초기화\n","W = torch.zeros((3, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)"],"metadata":{"id":"LTVeU4dktZsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hypothesis Functon\n","# H(x) = W * x + b\n","# 입력변수가 n 개라면 weight 의 개수도 n 개이다.\n","# H(x) = w_1 * x_1 + w_2 * x_2 + w_3 * x_3 +......+w_n * x_n + b\n","# matmul() 함수를 이용\n","hypothesis = x_train.matmul(W) + b    # or .mm or @"],"metadata":{"id":"LqiLf26hpc2v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cost Function: MSE\n","# 기존 Simple Linear Regression 과 동일하다.\n","cost = torch.mean((hypothesis - y_train)**2)"],"metadata":{"id":"FZg9RR0UqgLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient Descent with torch.optim\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr = 1e-5)\n","\n","#optimizer 사용법\n","optimizer.zero_grad()\n","cost.backward()\n","optimizer.step()    #각 layer의 파라미터와 같이 저장된 gradient 값을 이용하여 파라미터를 업데이트 한다."],"metadata":{"id":"TrHueccorWOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Full Code with torch.optim\n","\n","# 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","# 모델 초기화\n","W = torch.zeros((3, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr = 1e-5)\n","\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","\n","  # H(x) 계산\n","  hypothesis = x_train.matmul(W) + b\n","\n","  # cost 계산\n","  cost = torch.mean((hypothesis - y_train)**2)\n","\n","  # cost 로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n","      epoch, nb_epochs, hypothesis.squeeze().detach(),    # 여기서 사용된 detach() 함수는 gradient 의 전파를 막는다.\n","      cost.item()   # 텐서에서 값만을 추출하여 scalar 값을 얻는다.\n","  ))"],"metadata":{"id":"Qz-0CfDstI47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn.Module\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MultivariateLinearRegressionModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Linear(3, 1)   # 입력 차원: 3, 출력차원: 1\n","\n","  def forward(self, x):     # Hypothesis 계산은 forward() 에서 수행\n","    return self.linear(x)"],"metadata":{"id":"BTzGu2ZRu0To"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Full Code with torch.optim\n","\n","## 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","# 모델 초기화\n","model = MultivariateLinearRegressionModel()\n","\n","# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr = 1e-5)\n","\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","\n","  # H(x) 계산\n","  hypothesis = model(x_train)\n","\n","  # cost 계산\n","  cost = F.mse_loss(hypothesis, y_train)\n","\n","  # cost 로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n","      epoch, nb_epochs, hypothesis.squeeze().detach(),    # 여기서 사용된 detach() 함수는 gradient 의 전파를 막는다.\n","      cost.item()   # 텐서에서 값만을 추출하여 scalar 값을 얻는다.\n","  ))"],"metadata":{"id":"yq7lIvyJymm4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qOc2oFOB0ijr"},"execution_count":null,"outputs":[]}]}