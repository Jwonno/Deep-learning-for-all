{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMe9wREXY68nSNlOAumbmgd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6cl-0Ts-cxZ"},"outputs":[],"source":["import torch\n","import torch.optim as optim"]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# for reproducibility\n","torch.manual_seed(777)\n","if device == 'cuda':\n","  torch.cuda.manual_seed_all(777)"],"metadata":{"id":"z_GLhu3lG-KQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Backporpagation\n","X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n","Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"],"metadata":{"id":"HT7VS39AHXLJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn Layers\n","w1 = torch.Tensor(2, 2).to(device)\n","w2 = torch.Tensor(2, 1).to(device)\n","b1 = torch.Tensor(2).to(device)\n","b2 = torch.Tensor(1).to(device)\n","\n","# Sigmoid Function\n","def sigmoid(x):\n","  return 1.0/(1.0 + torch.exp(-x))\n","\n","# Derivative of Sigmoid Function\n","def sigmoid_prime(x):\n","  return sigmoid(x) * (1 - sigmoid(x))"],"metadata":{"id":"i0WXRR2NHlJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# learning rate\n","learning_rate = 0.5"],"metadata":{"id":"8mcDBn1iO55o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for step in range(10001):\n","  # forward()\n","  l1 = torch.add(torch.matmul(X, w1), b1)\n","  a1 = sigmoid(l1)\n","  l2 = torch.add(torch.matmul(a1, w2), b2)\n","  Y_pred = sigmoid(l2)\n","\n","  # Binary Cross Entropy\n","  cost = -torch.mean(Y * torch.log(Y_pred) + (1- Y) * torch.log(1 - Y_pred))\n","\n","  # backpropagation (chain rule)\n","  # Loss derivative\n","  d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)      # 1e-7 은 0으로 나눠지는 것을 방지하기 위해 추가함\n","\n","  # Layer 2\n","  d_l2 = d_Y_pred * sigmoid_prime(l2)\n","  d_b2 = d_l2\n","  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n","  # transpose() 함수는 첫번째 인자의 Tensor 를 두번째 인자와 세번째 인자의 차원축을 바꾸는 기능을 한다.\n","  # 즉 10 * 5 Tensor 가 있고 transpose(Tensor, 0, 1) 을 적용하면, 5 * 10 Tensor 가 되는 것이다.\n","\n","  # Layer 1\n","  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n","  d_l1 = d_a1 * sigmoid_prime(l1)\n","  d_b1 = d_l1\n","  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n","\n","  # weight update\n","  w1 = w1 - learning_rate * d_w1                  # gradient ascent 를 하려면 + 로 바꾸면 됨\n","  b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n","  w2 = w2 - learning_rate * d_w2\n","  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n","\n","  if step % 100 == 0:\n","    print(step, cost.item())"],"metadata":{"id":"s6OnpyTqH65J"},"execution_count":null,"outputs":[]}]}