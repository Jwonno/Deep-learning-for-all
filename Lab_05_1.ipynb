{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP/qBw6lJTYBjpMGfrdKqDF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wqRjbY-aEtWT"},"outputs":[],"source":["# Imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","source":["# For reproducibility\n","# 매 실험마다 같은 결과값을 도출해내기위해 시드값을 고정\n","torch.manual_seed(1)"],"metadata":{"id":"pZC5cPmlE8KJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Data\n","# x_data 는 각각의 학생들이 강의를 시청한 시간과 code lab 에서 보낸 시간을 의미한다.\n","# y_data 는 그에 따른 학생들의 course 통과여부를 의미한다.\n","x_data = [[1, 2],\n","          [2, 3],\n","          [3, 1],\n","          [4, 3],\n","          [5, 3],\n","          [6, 2]]\n","y_data = [[0],\n","          [0],\n","          [0],\n","          [1],\n","          [1],\n","          [1]]\n","\n","# 이 데이터들을 torch.Tensor 로 변환\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)\n","\n","# 데이터의 shape 확인\n","print(x_train.shape)\n","print(y_train.shape)"],"metadata":{"id":"uKPlaff6FPgy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing the Hypothesis\n","\n","# exponential function 확인\n","print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))\n","\n","# torch.exp() 를 이용하여 hypothesis function 구현\n","W = torch.zeros((2, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","hypothesis = 1/(1 + torch.exp(-(x_train.matmul(W) + b)))\n","\n","print(hypothesis)     # 현재 W 와 bias 의 모든 element 는 0 이므로 0.5 를 출력한다.\n","print(hypothesis.shape)   # x_train 과 W 의 shape 가 각각  6*2, 2*1 이므로 6*1 의 shape 를 가진다."],"metadata":{"id":"hn4f21IAGKLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# More simpler method, torch.sigmoid()\n","\n","# Sigmoid function 확인\n","print('1/(1+e^{-1}) equals: ', torch.sigmoid(torch.FloatTensor([1])))\n","\n","# torch.sigmoid() 를 이용하여 더 간단한 hypothesis function 구현\n","hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","print(hypothesis)\n","print(hypothesis.shape)"],"metadata":{"id":"mT2IdZyYI2eI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Computing the Cost Function (Low-level)\n","\n","print(hypothesis)\n","print(y_train)\n","\n","# 한 개의 원소에 대해 loss 는 다음과 같이 계산된다.\n","-(y_train[0] * torch.log(hypothesis[0]) +\n","  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))\n","# hypothesis[0] 이 1 일 확률이라면 (1 - hypothesis) 는 0 일 확률이다.\n","\n","# 전체 batch 에 대해 losses 를 계산하면 다음과 같다.\n","losses = -(y_train * torch.log(hypothesis) +\n","           (1 - y_train) * torch.log(1 - hypothesis))\n","print(losses)   # 6 * 1\n","\n","# .mean() 을 이용하여 각각의 losses 의 평균을 구한다.\n","cost = losses.mean()\n","print(cost)   # scalar"],"metadata":{"id":"CUreE0l1K_Qk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Computing the Cost function with F.binary_cross_entropy\n","# 위의 과정을 모두 포함하여 cost 를 구하는 method F.binary_cross_entropy 가 있다.\n","\n","F.binary_cross_entropy(hypothesis, y_train)   # equals: cost"],"metadata":{"id":"x5lGmfg8LMyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training with Low-Level Binary Cross Entropy Loss\n","\n","x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n","y_data = [[0], [0], [0], [1], [1],[1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)"],"metadata":{"id":"w7muveAINBnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 초기화\n","W = torch.zeros((2, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","  # Cost 계산\n","  hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","  cost = -(y_train * torch.log(hypothesis) +\n","          (1 - y_train) * torch.log(1 - hypothesis)).mean()\n","\n","  # Cost 로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  # 100번 마다 로그 출력\n","  if epoch % 100 == 0:\n","    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, cost.item()\n","    ))"],"metadata":{"id":"32skHUpKNdSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading Real Data\n","# 실제 데이터를 load 함\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import numpy as np\n","\n","xy = np.loadtxt('/content/drive/MyDrive/AI_class/Deep_learning_class/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)\n","\n","print(x_train[0:5])   # 5 개의 데이터만 출력\n","print(y_train[0:5])"],"metadata":{"id":"eIznUslaOpyp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training with Real Data using low-level Binary Cross Entropy Loss\n","# 모델 초기화\n","W = torch.zeros((8, 1), requires_grad=True)     # 위에서 load 한 실제 데이터의 dimension 이 8 이므로 W 는 8 * 1 의 shape 를 가진다.\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","  # Cost 계산\n","  hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","  cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()\n","\n","  # cost 로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  # 10 번 마다 로그 출력\n","  if epoch % 10 == 0:\n","    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, cost.item()\n","    ))"],"metadata":{"id":"1UtudqdRREgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training with Real Data using F.binary_cross_entropy\n","# 모델 초기화\n","W = torch.zeros((8, 1), requires_grad=True)     # 위에서 load 한 실제 데이터의 dimension 이 8 이므로 W 는 8 * 1 의 shape 를 가진다.\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","  # Cost 계산\n","  hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","  cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","  # cost 로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  # 10 번 마다 로그 출력\n","  if epoch % 10 == 0:\n","    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, cost.item()\n","    ))"],"metadata":{"id":"_L6jvsJXVwfw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the Accuracy our Model\n","\n","# hypothesis 의 Tensor 를 출력해보면 다음과 같다.\n","hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","print(hypothesis[:5])   # 처음부터 5개의 데이터를 출력"],"metadata":{"id":"X6dIvOLeWHJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 출력된 hypothesis 를 binary predictions 으로 바꾼다.(0.5 를 기준으로)\n","prediction = (hypothesis >= torch.FloatTensor([0.5])) # Boolean equation 이므로 prediction 에 저장된 값은 True or False 이다.\n","print(prediction[:5].type(torch.uint8))"],"metadata":{"id":"gJdg9kmgWpfz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# correct_prsdiction 에 이 값을 y_train 값과 비교하여 일치하면 1, 일치하지 않으면 0 을 저장한다.\n","print(prediction[:5].type(torch.uint8))\n","print(y_train[:5])\n","\n","correct_prediction = prediction.float() == y_train\n","print(correct_prediction[:5].type(torch.uint8))\n","\n"],"metadata":{"id":"mXKG_AUmXYY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델의 정확도 출력\n","accuracy = correct_prediction.sum().item()/len(correct_prediction)\n","print('The model has an accuracy of {:2.2f}% for the training set.'.format(accuracy * 100))\n","# 백분율로 나타냈으므로 accuracy 에 100 을 곱하여 출력했다."],"metadata":{"id":"Hryi5YQ8ZUTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# High-level Implementation with nn.Module\n","\n","class BinaryClassifier(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Linear(8, 1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    return self.sigmoid(self.linear(x))\n","\n","model = BinaryClassifier()\n","\n","# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=1)\n","\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","  # H(x) 계산\n","  hypothesis = model(x_train)\n","\n","  # Cost 계산\n","  cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","  # cost로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  # 10 번 마다 로그 출력\n","  if epoch % 10 == 0:\n","    prediction = hypothesis >= torch.FloatTensor([0.5])\n","    correct_prediction = prediction.float() == y_train\n","    accuracy = correct_prediction.sum().item() / len(correct_prediction)\n","    print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n","        epoch, nb_epochs, cost.item(), accuracy * 100,\n","    ))"],"metadata":{"id":"ze_uKyhpacAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2GVbkqP9g6OJ"},"execution_count":null,"outputs":[]}]}