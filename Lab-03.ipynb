{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP7+6WTcQ3QMaGlL/pnMI+R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt"],"metadata":{"id":"PwTECppiAi72"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sHG6NWu_1cb"},"outputs":[],"source":["# Simpler Hypothesis Function (without bias)"]},{"cell_type":"code","source":["# Dummy Data\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[1], [2], [3]])"],"metadata":{"id":"R7xdvtctANlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hypothesis\n","W = torch.zeros(1)\n","# b = torch.zeros(1, requires_grad=True)  bias 가 없는 더 simple 한 모델\n","hypothesis = x_train * W"],"metadata":{"id":"d8uz_sneBtYr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cost function: Intuition\n","# W = 1 일 때 cost = 0 이고 1 에서 멀어질수록 cost 는 증가한다.\n","# 잘 학습한 데이터일수록 cost 가 낮다."],"metadata":{"id":"KwMF5UFmAhkd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cost function: MSE(Mean Squared Error)\n","cost = torch.mean((hypothesis-y_train)**2)"],"metadata":{"id":"3988db1gA5tF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient Descent: Intuition\n","x_graph = np.array(np.linspace(-4, 6))\n","\n","plt.xlabel('W')\n","plt.ylabel('Cost')\n","\n","plt.plot(x_graph, (x_graph-1)*(x_graph-1)+1)\n","# 기울기가 가파를수록 cost 가 커지고, 기울기가 평평할수록 cost 가 작아진다.\n","# \"Gradient\"를 계산해야한다."],"metadata":{"id":"aQZI_iNxBHwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient Descent: Math\n","# W: W - alpha(learning rate) * gradient W"],"metadata":{"id":"FACkkPmVFW7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient Descent: Code\n","gradient = 2 * torch.mean((W * x_train - y_train) * x_train)\n","lr = 0.1    # lr = learning rate\n","W -= lr * gradient"],"metadata":{"id":"de6rjpRXE4f8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Full Code\n","# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[1], [2], [3]])\n","# 모델 초기화\n","W = torch.zeros(1)\n","# learning rate 설정\n","lr = 0.1\n","\n","nb_epochs = 10        # nb_epochs: 총 반복횟수    epochs: 현재 반복횟수\n","for epochs in range(nb_epochs + 1):\n","\n","  # H(x) 계산\n","  hypothesis = x_train * W\n","\n","  # cost gradient 계산\n","  cost = torch.mean((hypothesis - y_train)**2)\n","  gradient = torch.sum((W * x_train - y_train) * x_train)\n","\n","  print('Epochs {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n","      epochs, nb_epochs, W.item(), cost.item()\n","  ))\n","\n","  # cost gradient 로 H(x) 개선\n","  W -= lr * gradient"],"metadata":{"id":"qwc4QpzpFjD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient Descent with torch.optim\n","# torch.optim 라이브러리를 이용해서도 gradient descent 가 가능하다.\n","\n","# optimizer 설정\n","# optimizer = torch.optim.SGD([W], lr=0.15)\n","\n","# cost로 H(x) 개선\n","# optimizer.zero_grad()   # 전부 0 으로 초기화\n","# cost.backward()          # cost function 을 미분해서 각 변수들을 gradient 값으로 채운다.\n","# optimizer.step()"],"metadata":{"id":"FWa-4d8QHPuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Full Code with torch.optim\n","# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[1], [2], [3]])\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = torch.optim.SGD([W], lr=0.15)\n","\n","nb_epochs = 10        # nb_epochs: 총 반복횟수    epochs: 현재 반복횟수\n","for epochs in range(nb_epochs + 1):\n","\n","  # H(x) 계산\n","  hypothesis = x_train * W\n","\n","  # cost gradient 계산\n","  cost = torch.mean((hypothesis - y_train)**2)\n","\n","  print('Epochs {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n","      epochs, nb_epochs, W.item(), cost.item()\n","  ))\n","\n","  # cost gradient 로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()"],"metadata":{"id":"ZZRZSexkIdRe"},"execution_count":null,"outputs":[]}]}