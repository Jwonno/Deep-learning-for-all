{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMquGuwa3E3h1X8U6k1lDIo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7_398gVa5yn7"},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"code","source":["# Data dfinition\n","# 데이터는 torch.tensor\n","# 입출력은 x, y 로 구분\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])"],"metadata":{"id":"4YD-1GDh6CTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hypothesis\n","# y = Wx + b    (W 는 weight, b 는 bias 이다.)\n","W = torch.zeros(1, requires_grad=True)    # 0으로 초기화\n","b = torch.zeros(1, requires_grad=True)    # 0으로 초기화\n","hypothesis = x_train * W + b"],"metadata":{"id":"XxEHwffa6gP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute loss\n","# 차이를 제곱해서 평균을 구하는 것\n","cost = torch.mean((hypothesis - y_train) ** 2)"],"metadata":{"id":"uflOHtgL7A5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient descent\n","# torch.optim 라이브러리 사용\n","# [W, b] 는 학습할 Tensor 들\n","# lr = 0.01 은 learning rate\n","\n","# 항상 같이 쓰이는 3줄\n","# zero_grad() 로 gradient 초기화\n","# backward() 로 gradient 계산\n","# step() 으로 개선\n","optimizer = torch.optim.SGD([W, b], lr=0.01)\n","optimizer.zero_grad()\n","cost.backward()\n","optimizer.step()"],"metadata":{"id":"rATdLjtG7om5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Full training code\n","#once(Data definition)\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","x_train = torch.FloatTensor([[2], [4], [6]])\n","\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","optimizer = torch.optim.SGD([W, b], lr=0.01)\n","\n","#multiple\n","nb_epochs = 1000\n","for epochs in range(1, nb_epochs + 1):\n","  hypothesis = x_train * W + b\n","  cost = torch.mean((hypothesis - y_train)**2)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()"],"metadata":{"id":"SZ-ARBsb8ZzS"},"execution_count":null,"outputs":[]}]}